{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For these questions, assume that an $x$ input has 1024 dimensions, that the first hidden layer should have $512$ units, a second layer has $256$ units, and that there are $10$ classes to choose from at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\grad}[1]{\\nabla #1}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do the rows of $X$ represent? What do the columns of $X$ represent? What is the shape of $X$?**\n",
    "\n",
    "Rows represent each image in the batch.\n",
    "\n",
    "Columns represent each pixel in the image; a column of values is the same pixel's value in each of the images.\n",
    "\n",
    "The shape of $X$ is $(bs, 1024)$. $bs$ is the \"batch size\", or the number of $x$ examples we want to simultaneously train on in our \"batch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have a first matrix of weights $W^{(1)}$ and a vector of biases $b^{(1)}$. What are the shapes of $W^{(1)}$ and $b^{(1)}$? Why have I written these superscripts?**\n",
    "\n",
    "The shape of $W^{(1)}$ is: $(1024, 512)$. The shape of $b^{(1)}$ is $(1, 512)$.\n",
    "\n",
    "This is the first set of weights and biases used to calculate the pre-activations for the first hidden layer. There will be more weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For a single $x$ input of shape $(1, 1024)$, what is the formula to calculate the hidden pre-activation values $z^{(1)}$? What is the dimensionality of $z^{(1)}$?**\n",
    "\n",
    "Formula is:\n",
    "\n",
    "\\\\[\n",
    "z^{(1)} = x W^{(1)} + b^{(1)}\n",
    "\\\\]\n",
    "\n",
    "The dimensionality of $z^{(1)}$ is $(1, 512)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For a batch input matrix $X$ of shape $(bs, 1024)$, what is the formula to calculate the hidden pre-activation values $Z^{(1)}$? What is the dimensionality of $Z^{(1)}$?**\n",
    "\n",
    "**Hint**: in numpy, if $A$ has dimension $(10, 24)$, then you can add $b$ a vector of dimension $(1, 24)$, and $A + b$ is the $(10, 24)$ matrix where each row of $A$ has the vector $b$ added to it. This is called a *broadcasting* addition. You may write the formula with $+$ interpreted to allow broadcasting addition.\n",
    "\n",
    "\\\\[\n",
    "Z^{(1)} = X W^{(1)} + b^{(1)}\n",
    "\\\\]\n",
    "\n",
    "The dimensionality of $Z^{(1)}$ is $(bs, 512)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I covert a $3:1$ odds of winning to a $75$% probability of winning? If the odds of an event are $odds:1$, how do I cover that to a percentage?**\n",
    "\n",
    "If the odds are $odds:1$, then the formula for $odds$ to $p$ is $p = \\frac{odds}{1 + odds}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The $\\sigma$ function turns a number from $(-\\infty, \\infty)$ into a number from $(0, 1)$ that can be used as a probability. What is the formula for $\\sigma(z)$? Give me the version with $e^z$ in the numerator.**\n",
    "\n",
    "$\\sigma(z) = \\frac{e^z}{1 + e^z}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's say $f(odds) = \\frac{odds}{1 + odds}$. $f$ converts and odds to a probability. Can you write sigmoid in terms of $f$?**\n",
    "\n",
    "$\\sigma(z) = f(e^z)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we usually interpret the input of $f$ as an odds, then if we try to interpret $e^z$ as an odds, what does that imply we would interpret $z$ as?**\n",
    "\n",
    "We interpret $z$ as the log of an odds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What $z$ value has $\\sigma(z) = 0.50$ (50% probability) equivalent to an odds of $1.0$?**\n",
    "\n",
    "$z = 0.0$.\n",
    "\n",
    "**When is the probability $<0.5$, when is the probability $>0.5$?**\n",
    "\n",
    "When $z$ is negative the probability will be less than half, when $z$ is positive probability will be greater than half.\n",
    "\n",
    "Note: $\\sigma(z)$ isn't necessarily a probability. It can be the \"percent activated.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the problem numerically with the $\\sigma(z) = \\frac{e^z}{1+e^z}$ formula? What happens when $z$ is really large? What will we calculate on our CPU? What do we *want* to calculate theoretically?**\n",
    "\n",
    "When $z$ is really large then the floating point representation of $e^z$ can overflow and be $\\infty$.\n",
    "\n",
    "That's a problem because both the numerator and denominator will be $\\infty$ which means their ratio is not a number. We want it to be: $1.0$.\n",
    "\n",
    "**Is there a problem for very negative $z$s with this formula?**\n",
    "\n",
    "No, because $e^z$ will round to $0.0$ and that's not a problem because this is zero divided by one which is zero which is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we fix this problem? What's the better formula**\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{e^{-z} + 1}$. It works for very negative and very positive $z$ values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I calculate the activation values $H^{(1)}$ from $Z^{(1)}$?**\n",
    "\n",
    "\\\\[\n",
    "H^{(1)} = \\sigma(Z^{(1)})\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we want to use this $\\sigma$ function?**\n",
    "\n",
    "One way to interpret is that hidden layer activations represent neurons that are \"percent-activated.\"\n",
    "\n",
    "Another way to look at this is that neural networks are *universal function approximators*. Any function $f: (0, 1) \\to \\mathcal{R}$ can be approximated by a weighted sum of \"step functions\". A step function is equal to $1.0$ on a range $(a, b)$ and zero outside.\n",
    "\n",
    "Additional hidden layers add no value if using only pure linear functions. The result would still be linear and representable by a single layer. Extra layers are a waste.\n",
    "\n",
    "Composing a series of nonlinear functions one after the other *can* result in functions that could not be represented with a single layer. That is, more layers can result in more sophisticated or complex functions. Or more expressive power.\n",
    "\n",
    "Any function can be approximated by a neural network with relu activations. This means neural networks are *universal function approximators*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is another name for a function like $\\sigma$ when used to calculate hidden activations? What are other examples?**\n",
    "\n",
    "Activation function. ReLU, softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do I call the linear transformation of the $X$ values before being input into the activation function? What symbols do I use?**\n",
    "\n",
    "We wrote it as $z^{(1)}$ and it is called pre-activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we notate the $i$th row of $W^{(1)}$? The $j$th column?**\n",
    "\n",
    "$W^{(1)}_{i, :}$ and $W^{(1)}_{:, j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the formula for calculating a specific pre-activation $z^{(1)}_j$ for a single input $x$? Write as a vector operation with a dot-product and also with an explicit sum using $\\sum$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "z^{(1)}_j &= x \\cdot W^{(1)}_{:, j} + b^{(1)}_j\n",
    "\\\\\n",
    "z^{(1)}_j &= \\left(\n",
    "    \\sum_{i = 0}^{1024} x_i W^{(1)}_{i, j}\n",
    "\\right) + b^{(1)}_j\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does a column of $W^{(1)}_{:, j}$ represent? What does a row of $W^{(1)}_{i, :}$ represent?**\n",
    "\n",
    "The column consists of weights for each input dimension $x_i$ used to calculate the preactivation $z^{(1)}_j$. They are the weights for the $j$th hidden unit.\n",
    "\n",
    "The row consists of weights for a single input dimension $x_i$ used to compute the contribution of $x_i$ to each of the hidden pre-activations $z^{(1)}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the dimensions of $W^{(2)}$ and $b^{(2)}$?**\n",
    "\n",
    "$(512, 256)$ and $(1, 256)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the dimension of the first-layer activations $H^{(1)}$?**\n",
    "\n",
    "$(bs, 512)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the formulas for $Z^{(2)}$ and $H^{(2)}$?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "Z^{(2)} &= H^{(1)} W^{(2)} + b^{(2)}\n",
    "\\\\\n",
    "H^{(2)} &= \\sigma(Z^{(2)})\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the row $W^{(2)}_{i, :}$ represent? What does the column $W^{(2)}_{:, j}$ represent?**\n",
    "\n",
    "The $j$th column of $W^{(2)}$ represents the weights for each output of the first hidden layer used to compute the $j$th pre-activation of the second hidden layer.\n",
    "\n",
    "The $i$th row of $W^{(2)}$ is all of the weights for the $i$th activation of the hidden layer $H^{(1)}$ used to compute the contribution of the $i$th hidden unit of the first layer all the hidden pre-activations $z^{(2)}_j$ for all $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the dimensions of $W^{(3)}, b^{(3)}$? What is the shape of $Z^{(3)}$?**\n",
    "\n",
    "$(256, 10)$ and $(1, 10)$. $(bs, 10)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the formula for calculating $Z^{(3)}$? What is the formula for calculating $H^{(3)}$? Hint: you don't use $\\sigma$ this time!**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "Z^{(3)} &= H^{(2)} W^{(3)} + b^{(3)}\n",
    "\\\\\n",
    "H^{(3)} &= softmax(Z^{(3)})\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The $softmax$ function maps a 10-dimensional vector $z^{(3)}$ to a 10-dimensional vector $h^{(3)}$. How is $h^{(3)}_i$ calculated?**\n",
    "\n",
    "\\\\[\n",
    "h^{(3)}_i = \\frac{\n",
    "    \\exp \\left(z^{(3)}_i\\right)\n",
    "}{\n",
    "    \\sum_{j = 0}^9 \\exp \\left(z^{(3)}_j\\right)\n",
    "}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is the $softmax$ function like the $\\sigma$ function?**\n",
    "\n",
    "**TODO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In our problem, we want to classify an input $x$ as one of ten classes. Let's represent the correct answer in our training dataset as $y_{\\text{scalar}}$. What is the shape of $y_{\\text{scalar}}$? What is its range of values?**\n",
    "\n",
    "Shape of $y_{\\text{scalar}}$ is `()` or just a scalar. The range is zero to nine.\n",
    "\n",
    "**What is the format for $y_{\\text{one_hot}}$? What is the shape and range of values?**\n",
    "\n",
    "This is a ten-dimensional vector, where all the values are zero, except at one position. At the position $y_{\\text{scalar}}$, the value is $1.0$.\n",
    "\n",
    "In a sense, the one-hot $y_{\\text{one_hot}}$ representation is a \"perfect\" probability distribution for the correct answer.\n",
    "\n",
    "**What is the format for $Y_{\\text{scalar}}$?**\n",
    "\n",
    "Shape of $Y_{\\text{scalar}}$ is $(bs,)$ a vector of length $bs$. And the range is zero to nine.\n",
    "\n",
    "**What is the format for $Y_{\\text{one_hot}}$?**\n",
    "\n",
    "Shape is $(bs, 10)$ and each row is a one hot encoding of $\\left(Y_{\\text{scalar}}\\right)_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What properties do we want out of our last hidden layer $h^{(3)}$ (aka, the output layer)?**\n",
    "\n",
    "All values $h^{(3)}_i$ must be between zero and one because otherwise they're not a valid probability.\n",
    "\n",
    "The probabilities should sum to one so that $h^{(3)}$ forms a proper probability *distribution.*\n",
    "\n",
    "For an input $x$, we ideally want $h^{(3)}$ to be equal to $y_{\\text{one_hot}}$: be all zeros except for at the position of the correct answer, where it has a value 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget about your $z$s and $h$s for a second.\n",
    "\n",
    "**If the probability we assign to the correct answer is $p$, then what is the ideal value of $p$?**\n",
    "\n",
    "1.0 or 100%.\n",
    "\n",
    "**What is the ideal value of $\\log p$?**\n",
    "\n",
    "0.0\n",
    "\n",
    "**What is the worst value of $p$? And $\\log p$?**\n",
    "\n",
    "0.0 and $-\\infty$.\n",
    "\n",
    "**If larger values of $p$ are better than what values of $\\log p$ are better?**\n",
    "\n",
    "Larger ones. Because monotonic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the properties of a loss function?**\n",
    "\n",
    "Loss function should be non-negative. Should be zero when perfect/correct.\n",
    "\n",
    "The worse the prediction the greater the loss function.\n",
    "\n",
    "**Can we use $\\log p$ by itself as a loss function? Why? What do we have to do to use $\\log p$ as a loss function?**\n",
    "\n",
    "No. Goes negative. Greater values are better.\n",
    "\n",
    "We use $-\\log p$ as the loss function.\n",
    "\n",
    "**Is there a deeper reason for using $-\\log p$ rather than some other random function like $2^{-\\log p} - 1$?**\n",
    "\n",
    "Is it simpler? Maybe. There's a deep reason related to maximum likelihood that we won't talk about tonight.\n",
    "\n",
    "**What do we call this loss function?**\n",
    "\n",
    "Cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I give you $h^{(3)}$ and a one-hot encoding $y$ for a single example, what is the formula for the cross-entropy loss?**\n",
    "\n",
    "**Note**: Unless I write $y_{\\text{scalar}}$, you may take me to mean $y_{\\text{one_hot}}$ whenever I write $y$ alone. Same for $Y$.\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "{CE}_{\\text{vector}} (h^{(3)}, y) &= -\\log\\left(\n",
    "    h^{(3)} \\cdot y\n",
    "\\right)\n",
    "\\\\\n",
    "&= -\\log\\left(\n",
    "    \\sum_{i = 0}^{9} h^{(3)}_i y_i\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you can write ${CE}_{\\text{vector}}$, can you write ${CE}_{\\text{matrix}}$? You may use a summation $\\sum$ over all rows of $H^{(3)}$ and $Y$, and you may use ${CE}_{\\text{vector}}$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "{CE}_{\\text{matrix}}(H^{(3)}, Y) = \\frac{1}{bs} \\sum_{i = 0}^{bs} {CE}_{\\text{vector}}\\left(\n",
    "    H^{(3)}_{i, :},\n",
    "    Y_{i, :}\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try to eliminate the explicit summation with $\\sum$ to take advantage of numpy broadcasting. First, let's write a formula using $H^{(3)}$ and $Y$ that produces a matrix where for each row $i$, all values are zero, except at position $(i, j)$, where $j = (Y_{\\text{scalar}})_i$. At that position, we want the value of $H^{(3)}_{i, j}$. You want to use coordinate-wise multiplication, not tradiational matrix multiplication.**\n",
    "\n",
    "\\\\[\n",
    "H^{(3)} * Y\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having computed this, write a formula using either `np.sum` or `np.max` that produces a vector of shape $(bs,)$, where each entry $i$ is equal to $H^{(3)}_{i, (Y_\\text{scalar})_i}$. Use the `axis` property.**\n",
    "\n",
    "\\\\[\n",
    "\\text{np.sum}(H^{(3)} * Y, axis = 1)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use this formula to calculate a vector of shape $(bs,)$ where each entry $i$ is equal to ${CE}_{\\text{vector}} (H^{(3)}_{i, :}, Y_{i, :})$.**\n",
    "\n",
    "\\\\[\n",
    "-\\log \\text{np.sum}(H^{(3)} * Y, axis = 1)\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last, use `np.sum` to calculate the total mean cross entropy for the batch.**\n",
    "\n",
    "\\\\[\n",
    "\\text{np.sum}\\left(\n",
    "    -\\log \\text{np.sum}(H^{(3)} * Y, axis = 1),\n",
    "    axis = 0\n",
    "\\right)\n",
    "/ bs\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation to $z^{(3)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is $\\fpartial{}{x} \\log x$?**\n",
    "\n",
    "$\\frac{1}{x}$\n",
    "\n",
    "**What is the derivative $\\fpartial{}{x} g(f(x))$ in terms of $\\fpartial{}{x} f(x)$ and $\\fpartial{}{x} g(x)$? Recall that just like you can use the same variable name in two functions and they have nothing to do with each other, the $x$ in the $\\fpartial{}{x} f(x)$ isn't necessarily the same as the $x$ in $\\fpartial{}{x} g(x)$.**\n",
    "\n",
    "Chain rule.\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{}{x} g\\left(f\\left(x\\right)\\right)\n",
    "=\n",
    "\\left(\\fpartial{}{y} g\\left(y\\right)\\right)\n",
    "    \\left(g\\left(x\\right)\\right)\n",
    "\\left(\\fpartial{}{x} f\\left(x\\right)\\right)\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to calculate $\\fpartial{}{z^{(3)}_i} CE_\\text{vector}(h^{(3)}, y)$. Recall that $h^{(3)} = \\text{SOFTMAX}\\left(z^{(3)}\\right)$. So before anything, let's write $CE_\\text{vector}(h^{(3)}, y)$ in terms of the $z^{(3)}$ by expanding the dot-product of $h^{(3)}$ and substituting the formula in terms of $z^{(3)}$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    CE\\left(h^{(3)}, y\\right)\n",
    "&=\n",
    "    -\\log h^{(3)} \\cdot y\n",
    "\\\\\n",
    "&=\n",
    "    -\\log \\sum_{i = 0}^{9} h^{(3)}_i y_i\n",
    "\\\\\n",
    "&=\n",
    "    -\\log \\sum_{i = 0}^{9}\n",
    "        y_i\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_i\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Next let the scalar value $y^*$ be the correct class. That is $y_i = 1$ exactly when $i = y^*$; else $y_i$ is zero.**\n",
    "\n",
    "**Notice then that all the terms of the sum don't matter except for $i = y^*$. So simplify the above formula**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    CE\\left(h^{(3)}, y\\right)\n",
    "&=\n",
    "    -\\log \\sum_{i = 0}^{9}\n",
    "        y_i\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_i\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "\\\\\n",
    "&=\n",
    "    -\\log\\left(\n",
    "        y_{y^*}\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_{y^*}\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "    \\right)\n",
    "\\\\\n",
    "&=\n",
    "    -\\log\\left(\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_{y*}\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Here we replace $y_{y^*}$ with 1 since this is the definition of the one-hot encoding.\n",
    "\n",
    "From here out, let's use $S := \\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)$ because I'm lazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's calculate $\\fpartial{}{z^{(3)}_i} CE_\\text{vector}(h^{(3)}, y)$ for $i \\ne y^*$. First, it will help to simplify a log of a fraction into a difference of logs.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    CE\\left(h^{(3)}, y\\right)\n",
    "&=\n",
    "    -\\log\\left(\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_{y*}\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "    \\right)\n",
    "\\\\\n",
    "&=\n",
    "    -\\log \\left(\\exp\\left(z^{(3)}_{y*}\\right)\\right)\n",
    "    +\n",
    "    \\log \\left(\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "    -z^{(3)}_{y*}\n",
    "    +\n",
    "    \\log \\left(\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "**Next, what is the derivative of the first term? Why?**\n",
    "\n",
    "Now, the first term is just $z^{(3)}_{y*}$, so if we differentiate with respect to some $z^{(3)}_i$ where $i$ is *not* the correct class, then this derivative is zero.\n",
    "\n",
    "Basically, a change to the other $z^{(3)}$ values doesn't change the numerator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the rule that the derivative of $\\log a$ wrt $a$ is $1/a$, and also the chain rule, do the first-step of the deriative of the second term wrt $z^{(3)}_i$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    \\fpartial{}{z^{(3)}_i}\n",
    "        \\log \\left(\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)\\right)\n",
    "&=\n",
    "    \\frac{1}{\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)}\n",
    "    \\fpartial{}{z^{(3)}_i}\n",
    "        \\left(\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "**Next, use the rule that the derivative of $e^a$ wrt $a$ is also $e^a$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    \\frac{1}{\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)}\n",
    "    \\fpartial{}{z^{(3)}_i}\n",
    "        \\left(\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)\\right)\n",
    "&=\n",
    "    \\frac{1}{\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)}\n",
    "    \\exp\\left(z^{(3)}_i\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "**Finally, use the definition of $h^{(3)}_i = SOFTMAX(z^{(3)})_i$ to simplify this.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    \\frac{1}{\\sum_{j=0}^9 \\exp\\left(z^{(3)}_j\\right)}\n",
    "    \\exp\\left(z^{(3)}_i\\right)\n",
    "&=\n",
    "    h^{(3)}_i\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, if $i = y^*$, what is the derivative of the first term of the difference of logs?**\n",
    "\n",
    "It is $-1.0$.\n",
    "\n",
    "**Does the formula for the derivative of the second difference term change?**\n",
    "\n",
    "No.\n",
    "\n",
    "**Thus, what is $\\fpartial{}{z^{(3)}_i} CE_\\text{vector}(h^{(3)}, y)$ when $i=y^*$?**\n",
    "\n",
    "$-1.0 + h^{3}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The gradient $\\grad_{z^{(3)}} CE_\\text{vector}(h^{(3)}, y)$ is a vector. What are its entries?**\n",
    "\n",
    "\\\\[\n",
    "\\grad_{z^{(3)}} CE_\\text{vector}(h^{(3)}, y)_i\n",
    "=\n",
    "\\fpartial{}{z^{(3)}_i} CE_\\text{vector}(h^{(3)}, y)\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the vectorized formula for $\\grad_{z^{(3)}} CE_\\text{vector}(h^{(3)}, y)$?**\n",
    "\n",
    "\\\\[\n",
    "\\grad_{z^{(3)}} CE_\\text{vector}(h^{(3)}, y)\n",
    "=\n",
    "h^{(3)} - y\n",
    "\\\\]\n",
    "\n",
    "Where $y$ is the one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do an intuition check. What entries of the gradient are positive? Which are negative? Why?**\n",
    "\n",
    "All entries except $y^*$ are positive, because increasing their log odds decreases the probability on the right answer. That increases the loss.\n",
    "\n",
    "Increasing $z^{(3)}_{y^*}$ increases the probability of the correct answer, so it reduces the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop to $W^{(3)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To update the weights $W^{(3)}$ we must calculate $\\grad_{W^{(3)}} CE_\\text{vector}(h^{(3)}, y)$. What is the shape of this \"2 dimensional gradient\"?**\n",
    "\n",
    "$(256, 10)$.\n",
    "\n",
    "**What does each entry $\\left(\\grad_{W^{(3)}} CE_\\text{vector}(h^{(3)}, y)\\right)_{i, j}$ mean?**\n",
    "\n",
    "It means how much the loss will change if we changed $W_{i, j}$ a little.\n",
    "\n",
    "**Reminder: which value in the second hidden layer does $W_{i, j}$ connect to which pre-activation in the third layer?**\n",
    "\n",
    "$h^{(2)}_i$ to $z^{(3)}_j$.\n",
    "\n",
    "**If for $j$ we have $\\fpartial{}{z^{(3)}_j} CE_\\text{vector}(h^{(3)}, y)$ is zero, what is $\\grad_{W^{(3)}} CE_\\text{vector}(h^{(3)}, y)$ for all $(i, j)$ for all $i$? Why?**\n",
    "\n",
    "It must be zero. Because changing any $W_{i, j}$ may change $z^{(3)}_j$, but we know that has no impact on the loss.\n",
    "\n",
    "**If for $i$ we have $h^{(2)}_i$ is zero, what is $\\grad_{W^{(3)}} CE_\\text{vector}(h^{(3)}, y)$ for all $(i, j)$ for all $j$? Why?**\n",
    "\n",
    "It must be zero. Changing any $W_{i, j}$ won't change $z^{(3)}_j$, because $z^{(3)}_j$ is a weighted sum of the $h^{(2)}_i$, but if a $h^{(2)}_i$ is zero, changing the associated weight doesn't change $z^{(3)}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given the above, what two \"forces\" does $\\left(\\grad_{W^{(3)}} CE_\\text{vector}(h^{(3)}, y)\\right)_{i, j}$ need to combine?**\n",
    "\n",
    "First, the amount that changing $W^{(3)}_{i, j}$ changes $z^{(3)}_j$.\n",
    "\n",
    "Second, the amount that changing $z^{(3)}_j$ changes the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the two forces combine for $\\left(\\grad_{W^{(3)}} CE_\\text{vector}(h^{(3)}, y)\\right)_{i, j}$? Remember that $CE$ is a function of $z^{(3)}_j$ and $z^{(3)}_j$ is a function of $W^{(3)}_{i, j}$. Something something chain rule? First calculate $\\fpartial{}{W^{(3)}_{i, j}} z^{(3)}_j$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    \\fpartial{}{W^{(3)}_{i, j}} z^{(3)}_j\n",
    "&=\n",
    "    \\fpartial{}{W^{(3)}_{i, j}} \\sum_{i=0}^{256} h^{(2)}_i W_{i, j}\n",
    "\\\\\n",
    "&=\n",
    "    h^{(2)}_i\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does it matter what $j$ is?\n",
    "2. Let's just calcualte the vector for $W^{(3)}_{:, j}$ for any $j$.\n",
    "3. What is the dimension of that vector.\n",
    "4. Something something outer product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
