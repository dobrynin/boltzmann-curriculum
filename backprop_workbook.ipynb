{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For these questions, assume that an $x$ input has 1024 dimensions, that the first hidden layer should have $512$ units, a second layer has $256$ units, and that there are $10$ classes to choose from at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do the rows of $X$ represent? What do the columns of $X$ represent? What is the shape of $X$?**\n",
    "\n",
    "Rows represent each image in the batch.\n",
    "\n",
    "Columns represent each pixel in the image; a column of values is the same pixel's value in each of the images.\n",
    "\n",
    "The shape of $X$ is $(bs, 1024)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have a first matrix of weights $W^{(1)}$ and a vector of biases $b^{(1)}$. What are the shapes of $W$ and $b$? Why have I written these superscripts?**\n",
    "\n",
    "The shape of $W^{(1)}$ is: $(1024, 512)$. The shape of $b^{(1)}$ is $(1, 512)$.\n",
    "\n",
    "This is the first set of weights and biases used to calculate the pre-activations for the first hidden layer. There will be more weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the formula to calculate the hidden pre-activation values $Z^{(1)}$? What is the dimensionality of $z^{(1)}$?**\n",
    "\n",
    "Formula is:\n",
    "\n",
    "\\\\[\n",
    "S^{(1)} = X W^{(1)} + b^{(1)}\n",
    "\\\\]\n",
    "\n",
    "The dimensionality of $Z^{(1)}$ is $(bs, 512)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the nice formula for the $\\sigma$ function? Why is this nice? (Hint: what is the formula to convert odds of two outcomes to probability?)**\n",
    "\n",
    "$\\sigma(z) = \\frac{e^z}{1 + e^z}$.\n",
    "\n",
    "If the odds are $odds:1$, then the formula for $odds$ to $p$ is $p = \\frac{odds}{1 + odds}$.\n",
    "\n",
    "**Let's say $f(odds) = \\frac{odds}{1 + odds}$. $f$ converts and odds to a probability. Can you write sigmoid in terms of $f$?**\n",
    "\n",
    "$\\sigma(z) = f(e^z)$\n",
    "\n",
    "**If we usually interpret the input of $f$ as an odds, then if we try to interpret $e^z$ as an odds, what does that suggest we interpret $z$ as?**\n",
    "\n",
    "We interpret $z$ as the log of an odds.\n",
    "\n",
    "**What z value has $\\sigma(z) = 0.50$ (50% probability) equivalent to an odds of $1.0$?**\n",
    "\n",
    "$z = 0.0$.\n",
    "\n",
    "**When is the probability $<0.5$, when is the probability $>0.5$?**\n",
    "\n",
    "When $z$ is negative the probability will be less than half, when $z$ is positive probability will be greater than half.\n",
    "\n",
    "**$\\sigma(z)$ isn't necessarily a probability. It can be the \"percent activated.\"**\n",
    "\n",
    "**What is the problem numerically with this formula?**\n",
    "\n",
    "When $z$ is really large then the floating point representation of $e^z$ can overflow and be $\\infty$.\n",
    "\n",
    "That's a problem because both the numerator and denominator will be $\\infty$ which means their ratio is not a number. We want it to be: $1.0$.\n",
    "\n",
    "**Is there a problem for very negative $z$s with this formula?**\n",
    "\n",
    "No, because $e^z$ will round to $0.0$ and that's not a problem because this is zero divided by one which is zero which is correct.\n",
    "\n",
    "**How do we fix this problem? What's the better formula**\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{e^{-z} + 1}$. It works for very negative and very positive $z$ values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I calculate the activation values $H^{(1)}$ from $Z^{(1)}$?**\n",
    "\n",
    "\\\\[\n",
    "H^{(1)} = \\sigma(Z^{(1)})\n",
    "\\\\]\n",
    "\n",
    "**Why do we want to use this $\\sigma$ function?**\n",
    "\n",
    "Additional hidden layers add no value if using only pure linear functions. The result would still be linear and representable by a single layer. Extra layers are a waste.\n",
    "\n",
    "Composing a series of nonlinear functions one after the other *can* result in functions that could not be represented with a single layer. That is, more layers can result in more sophisticated or complex functions. Or more expressive power.\n",
    "\n",
    "Any function can be approximated by a neural network with relu activations. This means neural networks are *universal function approximators*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is another name for the function $\\sigma$ when used to calculate hidden activations?**\n",
    "\n",
    "Activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do I call the linear transformation of the $X$ values before being input into the activation function? What symbols do I use?**\n",
    "\n",
    "We wrote it as $z^{(1)}$ and it is called pre-activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we notate the $i$th row of $W^{(1)}$? The $j$th column?**\n",
    "\n",
    "$W^{(1)}_{i, :}$ and $W^{(1)}_{:, j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the formula for calculating a specific pre-activation $z^{(1)}_j$ for a single input $x$?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "z^{(1)}_j &= x \\cdot W^{(1)}_{:, j} + b^{(1)}_j\n",
    "\\\\\n",
    "z^{(1)}_j &= \\left(\n",
    "    \\sum_{i = 0}^{1024} x_i W^{(1)}_{i, j}\n",
    "\\right) + b^{(1)}_j\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does a column of $W^{(1)}_{:, j}$ represent? What does a row of $W^{(1)}_{i, :}$ represent?**\n",
    "\n",
    "The column consists of weights for each input dimension $x_i$ used to calculate the preactivation $z^{(1)}_j$. They are the weights for the $j$th hidden unit.\n",
    "\n",
    "The row consists of weights for a single input dimension $x_i$ used to compute each of the hidden pre-activations $z^{(1)}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the dimensions of $W^{(2)}$ and $b^{(2)}$?**\n",
    "\n",
    "$(512, 256)$ and $(1, 256)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the dimension of the first-layer activations $H^{(1)}$?**\n",
    "\n",
    "$(bs, 512)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the formulas for $Z^{(2)}$ and $H^{(2)}$?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "Z^{(2)} &= H^{(1)} W^{(2)} + b^{(2)}\n",
    "\\\\\n",
    "H^{(2)} &= \\sigma(H^{(2)})\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the row $W^{(2)}_{i, :}$ represent? What does the column $W^{(2)}_{:, j}$ represent?**\n",
    "\n",
    "The $j$th column of $W^{(2)}$ represents the weights for each output of the first hidden layer used to compute the $j$th pre-activation of the second hidden layer.\n",
    "\n",
    "The $i$th row of $W^{(2)}$ is all of the weights for the $i$th activation of the hidden layer $H^{(1)}$ used to compute all the hidden pre-activations $z^{(2)}_j$ for all $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the dimensions of $W^{(3)}, b^{(3)}$? What is the shape of $Z^{(3)}$?**\n",
    "\n",
    "$(256, 10)$ and $(1, 10)$. $(bs, 10)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the formula for calculating $Z^{(3)}$? What is the formula for calculating $H^{(3)}$ (careful)?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "Z^{(3)} &= H^{(2)} W^{(3)} + b^{(3)}\n",
    "\\\\\n",
    "H^{(3)} &= softmax(Z^{(3)})\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What properties do we want out of the last hidden layer (aka, the output layer)?**\n",
    "\n",
    "They must be between zero and one because otherwise they're not valid as a probability at all.\n",
    "\n",
    "The probabilities should sum to one so that they form a probability *distribution.*\n",
    "\n",
    "For an input $x$, we want $h^{(3)}$ to be all zeros except at position $y$ which is the correct class. That value should be ideally 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the shape of $y$ for a single example? What is the range of $y$ values?**\n",
    "\n",
    "Shape of $y$ is `()` or just a scalar. The range is zero to nine.\n",
    "\n",
    "**What is the shape of $y$ for a batch of examples (I'm using the same symbol)? What is the range of $y$ values?**\n",
    "\n",
    "Shape of $y$ is $(bs,)$ a vector of length $bs$. And the range is zero to nine.\n",
    "\n",
    "**What is the matrix $Y$? What is the range of $Y$ values? What is the shape of $Y$?**\n",
    "\n",
    "Shape is $(bs, 10)$ and each row is a one hot encoding of $y_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget about your $z$s and $h$s for a second.\n",
    "\n",
    "**If the probability we assign to the correct answer is $p$, then what is the ideal value of $p$?**\n",
    "\n",
    "1.0 or 100%.\n",
    "\n",
    "**What is the ideal value of $\\log p$?**\n",
    "\n",
    "0.0\n",
    "\n",
    "**What is the worst value of $p$? And $\\log p$?**\n",
    "\n",
    "0.0 and $-\\infty$.\n",
    "\n",
    "**If larger values of $p$ are better than what values of $\\log p$ are better?**\n",
    "\n",
    "Larger ones. Because monotonic.\n",
    "\n",
    "**What are the properties of a loss function?**\n",
    "\n",
    "Loss function should be non-negative. Should be zero when perfect/correct.\n",
    "\n",
    "The worse the prediction the greater the loss function.\n",
    "\n",
    "**Can we use $\\log p$ by itself as a loss function? Why? What do we have to do to use $\\log p$ as a loss function?**\n",
    "\n",
    "No. Goes negative. Greater values are better.\n",
    "\n",
    "We use $-\\log p$ as the loss function.\n",
    "\n",
    "**Is there a deeper reason for using $-\\log p$ rather than some other random function like $2^{-\\log p} - 1$?**\n",
    "\n",
    "Is it simpler? Maybe. There's a deep reason related to maximum likelihood that we won't talk about tonight.\n",
    "\n",
    "**What do we call this loss function?**\n",
    "\n",
    "Cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I give you $h^{(3)}$ and a one-hot encoding $y$ for a single example, what is the formula for the cross-entropy loss?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "CE(h^{(3)}, y) &= -\\log\\left(\n",
    "    h^{(3)} \\cdot y\n",
    "\\right)\n",
    "\\\\\n",
    "&= -\\log\\left(\n",
    "    \\sum_{i = 0}^{9} h^{(3)}_i y_i\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you write the mean cross entropy for a batch using rows of the matrix $H^{(3)}$ and rows of the matrix $Y$ and doing a summation?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "CE(H^{(3)}, Y) = \\frac{1}{bs} \\sum_{i = 0}^{bs} CE\\left(\n",
    "    H^{(3)}_{i, :},\n",
    "    Y_{i, :}\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: ask how to do this with just vectors first.\n",
    "\n",
    "TODO: how do we get the dot product of the rows of $H^{(3)}$ and the rows of $Y$ using coordinate wise multiplication and summing of rows of a matrix to produce a vector.\n",
    "\n",
    "Make more clear: no \"real\" matrix operations or transposes or whatever.\n",
    "\n",
    "\\\\[\n",
    "\\text{np.sum}(H^{(3)} * Y, axis = 1)\n",
    "\\\\]\n",
    "\n",
    "This is a vector of length $bs$. And the values are dot products of corresponding rows, which is in fact the probability assigned to the correct answer.\n",
    "\n",
    "Next step is: take negative log of the vector. This produces a new vector of length $bs$.\n",
    "\n",
    "Last step is:\n",
    "\n",
    "\\\\[\n",
    "\\text{np.sum}(-\\log \\text{np.sum}(H^{(3)} * Y, axis = 1), axis = 0) / bs\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is $\\fpartial{}{x} \\log x$?**\n",
    "\n",
    "$\\frac{1}{x}$\n",
    "\n",
    "**What is the derivative $\\fpartial{}{x} g(f(x))$ in terms of $\\fpartial{}{x} f(x)$ and $\\fpartial{}{x} g(x)$? Recall that $x$ and $y$ are just variable names and are otherwise meaningless. Just like you can use the same variable name in two functions and they have nothing to do with each other, the $x$ in the $\\fpartial{}{x} f(x)$ isn't necessarily the same as the $x$ in $\\fpartial{}{x} g(x)$.**\n",
    "\n",
    "Chain rule.\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{}{x} g\\left(f\\left(x\\right)\\right)\n",
    "=\n",
    "\\left(\\fpartial{}{y} g\\left(y\\right)\\right)\n",
    "    \\left(f\\left(x\\right)\\right)\n",
    "\\left(\\fpartial{}{x} f\\left(x\\right)\\right)\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate $\\fpartial{}{z^{(3)}_i} CE(h^{(3)}, y)$. Recall that $h^{(3)} = \\text{SOFTMAX}\\left(z^{(3)}\\right)$. So first write CE in terms of the $z^{(3)}$ and then differentiate.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    CE\\left(h^{(3)}, y\\right)\n",
    "&=\n",
    "    -\\log h^{(3)} \\cdot y\n",
    "\\\\\n",
    "&=\n",
    "    -\\log \\sum_{i = 0}^{9} h^{(3)}_i y_i\n",
    "\\\\\n",
    "&=\n",
    "    -\\log \\sum_{i = 0}^{9}\n",
    "        y_i\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_i\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Next let $c$ be the correct class. Notice that all the log probabilities for the wrong classes don't matter so we have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    CE\\left(h^{(3)}, y\\right)\n",
    "&=\n",
    "    -\\log \\sum_{i = 0}^{9}\n",
    "        y_i\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_i\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "\\\\\n",
    "&=\n",
    "    -\\log\\left(\n",
    "        y_c\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_c\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "    \\right)\n",
    "\\\\\n",
    "&=\n",
    "    -\\log\\left(\n",
    "        \\frac{\n",
    "            \\exp\\left(z^{(3)}_c\\right)\n",
    "        }{\n",
    "            \\sum_{j=0}^9\n",
    "            \\exp\\left(z^{(3)}_j\\right)\n",
    "        }\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Here we replace $y_c = 1$ since this is the definition of the one-hot encoding.\n",
    "\n",
    "Let's use $S := \\exp\\left(z^{(3)}_j\\right)$ because I'm lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
